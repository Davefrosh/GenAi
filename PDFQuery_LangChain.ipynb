{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_FeN-Ep4Rpp"
      },
      "source": [
        "Install the required dependencies:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQN-L2J4Rpq"
      },
      "source": [
        "Import the packages you'll need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V4qBIihE4Rpq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\NEW USER\\anaconda3\\envs\\genv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# LangChain components to use\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Support for dataset retrieval with Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
        "# you will also initialize the DB connection:\n",
        "import cassio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs76OPQ6JyD",
        "outputId": "2464981f-aea9-499d-ccb4-5f43ea76061d"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1itBNL1v6N9-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='Comparative Analysis of Machine Learning Algorithms in Predicting Rate of \\nPenetration during Drilling\\nOlaosebikan Abidoye Olafadehan*, Ikenna David Ahaotu\\nDepartment of Chemical and Petroleum Engineering, University of Lagos, Akoka-Y aba, Lagos 101017, Nigeria\\nCitation: Olafadehan OA, Ahaotu  ID, Comparative Analysis of Machine Learning Algorithms in Predicting Rate of Penetration \\nduring Drilling. J Petro Chem Eng 2023;1(1): 32-47.\\nReceived: 14 October, 2023; Accepted: 31 October, 2023; Published: 07 November, 2023\\n*Corresponding author: Olafadehan OA, Department of Chemical and Petroleum Engineering, University of Lagos, Akoka-\\nY aba, Lagos 101017 Nigeria, Phone: +234802-912-9559, Email: oolafadehan@unilag.edu.ng\\nCopyright: © 2023 Olafadehan OA., et al., This is an open-access article published in J Petro Chem Eng (JPCE) and distributed \\nunder the terms of the Creative Commons Attribution License, which permits unrestricted use, dist ribution, and reproduction \\nin any medium, provided the original author and source are credited.\\n1\\n A B S T R A C T \\nDrilling for potential oil and gas reserves is one of the foremost practices in the petroleum industry. The dr illing process, \\nhowever, is quite expensive and can take quite some time to accomplish.  Hence, there has been a rise in the n eed to reduce \\ncost and time by optimizing the rate of penetration during drilling, which has led to the development of mathemat ical models \\nto describe and evaluate this process.  However, the accuracy of these models has varied owing to variat ion of the drilling \\nparameters accounted for in each model. This event has led to the usage of alternative approaches such as Data driven models.  In \\nthis study, the predictive capacities of the rate of penetration (ROP) during drilling using machine learning (ML) algorithms of \\nsupport vector machine regression (SVR), Random Forest regression (RF), Linear regression (LR), KNearest neighbors (KNN), \\nStacking technique, Voting technique and Convolution neural network (CNN), were compared.  Data from an oil well in Nigeria \\nwas used in this investigation. The data for the well was split into train–test sets in the ratio of 60:40.  The train data was used to \\ntrain and select the best model before making predictions on the test sets.  The Stacking technique was found t o have the best \\nperformance across both training and test data sets with respective accuracies of 99.8% and 97.5% in terms of the  –score. The \\nVoting technique also performed well, with respective accuracies of 93.6% and 92.6% in terms of the  –score across both sets of \\ndata. The CNN model equally performed well on the training and test data sets, with respective accuracies of 92.4% a nd 92.8% \\nin terms of the  –Score.  Generally, the machine learning models were able to detect patterns and gain valuable insights into the \\ndata. They can be employed for real time prediction of the rate of penetration during oil well drilling.\\nKeywords: Rate of penetration; Drilling; Artificial intelligence; Machine learning algorithms; train–test data.\\nAbbreviations\\nAI  Artificial Intelligence\\nANN            Artificial Neural Network\\nCNN            Convolutional Neural Network\\nDDR             Daily Drilling Report\\nKNN              KNearest Neighbors\\nMAE             Mean Absolute Error\\nML                  Machine Learning\\nR2                    Coefficient of determination\\nRMSE             Root mean squared Error.\\nROP                 Rate of penetration. \\nRPM            Rotary speed,\\nSVR                Support Vector Regressor\\nWOB              Weight on bit, kblf\\nResearch Article\\nVol: 1 & Iss: 1\\nhttps://urfpublishers.com/journal/petrochemical-engineering\\nJournal of Petroleum & Chemical Engineering'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n2\\n1. Introduction\\nDrilling is a key aspect of the petroleum industry. It is the \\nprocess of boring a hole deep into the subsurface section of the \\nearth in order to reach formations with hydrocarbon reserves, \\nfor the aim of hydrocarbon recovery. The importance of this \\nprocess cannot be understated and as a result, a lot of different \\ndrilling technologies were implemented to maximize drilling \\noperations. The popular drilling method used today known \\nas the rotary drilling, which is applied in drilling the majority \\nof onshore and offshore wells and makes use of an applied \\naxial force on the rotating drill bit to achieve penetration. It is \\nimpossible to overstate the significance of this procedure, which \\nis why numerous drilling methods have been used to maximize \\ndrilling operations. The bulk of onshore and offshore wells are \\ndrilled using the widely used technique known as the rotary \\ndrilling, which applies an axial force to the revolving drill bit \\nto accomplish penetration. In a rotary drilling process, key \\nparameters need to be considered to ensure optimal operations, \\nand a key parameter among these is the rate of penetration, ROP. \\nIt is the depth of penetration accomplished per unit time, and is \\nusually measured as a factor of how many feet the bit can drill in \\nan hour (i.e., ft/h). However, evaluation of ROP is difficult due \\nto the complex relationship between other drilling parameters \\naffecting the ROP. The rate of penetration (ROP) prediction is \\na key task in drilling economical assessments\\n1. Not always is \\nthe lowest cost per foot provided by the fastest drilling pace. A \\nrise in the project’s overall cost may be caused by other factors. \\nThe characteristics of drilling fluid (such as mud viscosity, mud \\ndensity, filtration loss), mechanical characteristics (such as bit \\ntype and weight), and formation properties (such as porosity, rock \\nabrasivity, formation elasticity, formation stress, permeability) \\nare a few examples of the properties that affect penetration rate\\n2. \\nHence, it is important to maximize the rate of penetration in order \\nto mitigate some of the general cost associated with drilling for \\nextended periods. Therefore, it is necessary to understand the \\nrelationship between the ROP and other operational parameters.\\nMathematical models have been used to model the \\nrelationship between some operational parameters and ROP \\ne.g., Bourgoyne and Young\\n3 model and the Bingham 4. The \\naccuracy of these models has varied due to variation in the \\ndrilling parameters considered in each model. This has led to \\nthe usage of alternative approaches such as a data driven model \\ne.g., artificial intelligence (AI). Artificial intelligence methods \\nhave developed rapidly over the past decades and has led to it \\nbeen implemented in various sectors, including the oil and gas \\nindustry. Colossal amount of data is been generated on the oil \\nfield during operating hours. These data include drilling data, \\nproduction data, seismic data and mud log data, amongst others. \\nThese data sets can be trained using artificial intelligence \\nmethods to make future predictions and generate hidden insights \\ninto the data. The AI methods have been used extensively in \\napplications to the petroleum industry where they can provide \\nsolutions to drilling problems such as prediction of drill bit wear \\nfrom drilling parameters, real-time predictions of alterations in \\ndrilling fluid rheology\\n5, and the estimation of oil recovery factor \\nfor water drive sandy reservoirs6.\\n1.1 Artificial Intelligence\\nMachine Learning (ML) and Deep Learning (DL) are \\nbranches of artificial intelligence that deals with computerized \\nsystems and algorithms learning from previous data generated\\n7. \\nBy utilizing various algorithmic strategies, they enable the \\nsystems to perform computational tasks without requiring \\nexplicit programming and learn from the data. Finding patterns'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='7. \\nBy utilizing various algorithmic strategies, they enable the \\nsystems to perform computational tasks without requiring \\nexplicit programming and learn from the data. Finding patterns \\nin numerical data by applying computer algorithms to convert \\ndata into numerical form is known as machine learning. \\nAmongst other formats, the data may be in the form of pictures, \\nmusic, numbers, or alphabetical data. The algorithms used to \\nfind the patterns within these data are called machine learning \\nmodels. These models, which include linear regression, logistic \\nregression, decision Trees, random forest, K-Means, K-Nearest \\nNeighbors, are used for prediction, data sub-grouping and \\nsound-detection, amongst others. They have been applied to \\naid in the prediction of ROP values with better accuracy and \\ngeneralization. ML operations are divided into supervised and \\nunsupervised learning. Supervised learning is a paradigm in \\nmachine learning here input objects and a desired output value \\ntrain a model. The training data is processed, and builds a \\nfunction that maps new data on expected output values (e.g., \\nregression and classification). In unsupervised learning, the data \\nhas no target label, the machine learning model aims at finding \\nhidden patterns in the data using algorithms to make critical \\njudgments in the future (e.g., clustering and recommendation).\\nDeep learning is a branch of the machine learning and \\nartificial intelligence that mimics the operation of how the human \\nbrain receives, process and transmit information, as depicted in \\nFigure 1.\\nFigure 1: Human neuron model.\\nDeep learning (DL) is essentially a neural network with one \\nor more layers. The components of the human neural network \\nare modelled similar to the neural network operation\\n8. The \\ndendrites act as input nodes, cell body represents activation \\nfunction, synapse is the weightage of each input, and the axon \\nterminal is the output node as shown in Figure 2.\\nFigure 2: A typical feed forward neural network architecture9.\\nNeural networks (or deep learning) are massively parallel'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='3\\nOlafadehan OA., et al., J Petro Chem Eng  | V ol: 1 & Iss: 1\\ndistributed processor that store and make use of experiential \\nknowledge. It is classified into 3 parts: artificial neural network \\n(ANN), convolutional neural network (CNN), and recurrent \\nneural network (RNN), which are used to carry out different \\noperations. The ANN is mostly used to carry out regression \\nand classification problems. The CNN is mostly used to carry \\nout image processing and prediction while the RNN is mostly \\nused for forecasting operations. A convolutional neural network \\nand a few machine learning strategies are used in this work. \\nConvolutional layers, feature extractors (filters), pooling layers, \\nhidden layers, and one or more output layers are the components \\nof a convolutional neural network. Weights are used to connect \\nthe layers in the hidden layer of the CNN structure. These weights \\nfacilitate information flow between layers and aid in neural \\nnetwork training. An activation function is present in every \\nhidden layer, which helps to save computational time and cost \\nby converting the data into a more computer-interactive format. \\nTo extract important features from the data, convolutional layers \\nassist in performing convolutional operations on the data.\\nBefore the data is sent to the filter, which extracts the features \\nand patterns in the dataset, the convolutional layer typically \\nreceives the input in the form of length, breadth, height, and \\ncolor channels. CNNs have two feature extraction layers: one \\nthat makes use of pooling layers and the other that makes use of \\nfilters. To extract even more important insights from the dataset, \\na pooling layer made up of a pooling approach is employed to \\nperform pooling on the features that the filter helped extract. To \\nconduct out-pooling, different sorts of pooling techniques are \\nemployed, such as MaxPooling, Average Pooling, and Global \\nPooling.\\nBilgesu et al\\n10. used an artificial neural network to develop an \\nROP model, which was dependent on several operating \\nparameters. A data of 500 points was used, with nine features, \\nwhich were tooth wear, rotary speed, torque, weight on bit, \\npump flow rate, rotating time, bearing wear, formation \\ndrillability, and formation abrasiveness. A train-test ratio of 9:1, \\nwhich implies 90% of the data was used for training and 10% for \\nvalidating the model. A coefficient of determination (\\n2R ) \\nbetween 0.902 and 0.982 was achieved after cross-validation \\nacross the data. In the work of Arabjamaloei and Shadizadeh\\n11, \\nan artificial neural network with a single hidden layer of 10 \\nneurons was developed and combined with genetic algorithm \\n(GA) to create a model to predict ROP values. There were seven \\nfeatures and 300 points (rows) in the data. The bit type, formation \\nproperties, bit operating condition (rotary speed and bit weight), \\nbit tooth wear, bit hydraulics, hydrostatic head, and equivalent \\ncirculating density were the input features. A total of 224 points \\nwere used for model training, 56 points for validation, and 20 \\npoints for testing. The generic algorithm was employed to find \\nwhere the maximum rate of penetration occurred. With a low \\nmean-square error for both training and test set, it was concluded \\nthat the neural network is valid for other data sets that fall within \\nthe range of data set used for training the model\\n12.performed a \\ncomparative evaluation of models for estimating the rate of \\npenetration (ROP) by utilizing field data from a well located in \\nIran. The model used for this study were the Bingham\\n4, Warren13 \\nand, Bourgoyne and Young 3 models. They carried out ROP \\npredictions on wells that were drilled with roller cone and PDC \\nbits, and comparison was carried out on three separate drilling \\nsections. However, there was a short coming of this study, in that \\nthreshold \\noBW was neglected due to lack of drill-off test been \\ncarried out. The findings of this study demonstrated that among'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='sections. However, there was a short coming of this study, in that \\nthreshold \\noBW was neglected due to lack of drill-off test been \\ncarried out. The findings of this study demonstrated that among \\nthe models examined, the Bourgoyne and Young model exhibited \\nthe highest level of predictive performance. Mahasneh\\n14 \\ndeveloped a mathematical model to predict the rate of penetration \\n(ROP) in gas wells, considering the factors of weight on bit \\n(WOB), bit rotation speed (RPM), flow rate (FR), formation \\nstrength, depth, and formation compaction. He then used his \\nmodel to optimize the drilling parameters for a gas well in \\nJordan, increasing the ROP by 15% and reducing the cost of \\ndrilling by 10%. Mahasneh\\n14’s study demonstrated the \\nimportance of drilling optimization in improving the efficiency \\nand cost-effectiveness of drilling operations. Amar and Ibrahim\\n15 \\nworked on the comparative analysis of physics-based equations \\nwith artificial neural networks (ANN). They developed two \\nneural network models to evaluate the ROP values. The input \\nparameters into the neural networks were formation depth, ECD, \\nweight on bit, DSR, pore pressure gradient, drill bit tooth wear, \\nand Reynolds number function. The physics-based equations \\nused for the comparative analysis were the Bingham\\n4 model and \\nBourgoyne and Young3 model. A comparison of the predictive \\naccuracy of the developed ANN-based models with the available \\nempirical equations showed that both ANN-based models were \\nhighly accurate for estimating the ROP as compared with the \\nempirical equations. Shi et al.\\n16predicted the rate of penetration \\n(ROP) using the Extreme Learning Machine (ELM) and Upper-\\nlayer solution-ware (USA) techniques. To construct the \\npredictive models, various input parameters such as formation \\nproperties, rig hydraulics, bit specifications, weight on bit, rotary \\nspeed, and mud properties were utilized. These input features \\nwere selected based on reservoir data from Bohai Bay, China. \\nThe performance of the developed models using ELM and USA \\ntechniques was compared with an artificial neural network \\nmodel. The accuracy of these models was evaluated using \\nmetrics such as regression coefficient (\\n2R ), mean absolute error \\n(MAE), and root mean square error ( RMSE). The findings \\nindicated that the ROP model developed with the USA technique \\nexhibited the highest predictive performance compared to the \\nother models. Additionally, it was observed that the development \\nof the ROP model using the extreme learning technique required \\nthe most time investment. Ahmed et al.\\n17 investigated the \\napplication of a support vector machine model to estimate the \\nrate of penetration in a formation containing shale materials. The \\ninput features used in the model were hinged on drilling \\nparameters and mud properties such as weight on bit, rotary \\nspeed, pump flow rate, standpipe pressure, drilling torque, mud \\ndensity, plastic viscosity, funnel viscosity, yield point and solid \\ncontent (%). The support vector machine model and the \\nBourgoyne and Youngs model were trained on more than 400 \\nreal data in shale formation using these 10 features as inputs. \\nThe two models were both compared on their predictive \\nperformance on the test data. The Bourgoyne and Young (1974) \\nmodel produced a coefficient of determination (\\n2R ) of 0.0692 \\nand an absolute percentage error of 23.41%. By applying the \\nsupport vector machine (SVM) model, a coefficient of \\ndetermination (\\n2R ) of 0.995 and an absolute percentage error of \\n2.82% were obtained. It was concluded that SVM can be used to \\npredict ROP with higher accuracy and also generate ROP values \\nfaster than the Bourgoyne and Young\\n3 model. Elkatany 5 \\ndeveloped an artificial neural network (ANN) model to predict \\nthe rate of penetration (ROP) using data collected from three \\nvertical wells in an offshore oilfield. The ANN-ROP model was \\nobtained based on drilling parameters and drilling fluid'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='the rate of penetration (ROP) using data collected from three \\nvertical wells in an offshore oilfield. The ANN-ROP model was \\nobtained based on drilling parameters and drilling fluid \\nproperties. Two wells were utilized for training the model, and \\nthe third well was used to evaluate the accuracy of the model.'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n4\\nThe performance of the ANN-ROP model was compared to \\nother ROP models of Bingham (1965), Bourgoyne and Young\\n3, \\nand Maurer18. Elkatany5 concluded that the proposed ANN-ROP \\nmodel exhibited superior performance over others considered in \\nhis work. The training data consisted of 3333 data points and \\nyielded a coefficient of determination (\\n2R ) of 0.99, with an \\naverage absolute percentage error ( AAPE) of 5%. The test set, \\nconsisting of 2700 unseen data points from the third well, \\nresulted in the ANN-ROP model predicting the rate of penetration \\nwith \\n99. 02 =R and AAPE = 4%. Zhang et al. 19 proposed a deep \\nconvolutional neural network (CNN) model for predicting the \\nrate of penetration (ROP) during drilling operations. The authors \\nargued that existing models for predicting ROP are often \\ninaccurate and unreliable, and that deep learning methods could \\nprovide a more accurate and practical solution. They collected \\ndata from drilling operations in two different fields and used it to \\ntrain and test the proposed deep CNN model in their work. The \\nmodel consists of six convolutional layers and is trained using a \\nmean absolute percentage error (MAPE) loss function. The \\nauthors compared the performance of their deep CNN model to \\nother machine learning models and found that it outperformed \\nthese models in terms of accuracy and reliability. They also \\nconducted sensitivity analyses to determine the most important \\nfeatures for predicting ROP. They found that the weight on bit, \\nthe rotary speed, and the mud flow rate were the most important \\nfeatures for predicting ROP. Zhao et al.\\n20 focused on developing \\nmultiple artificial neural network (ANN) models for predicting \\nthe rate of penetration (ROP) using data collected from a gas \\nwell located in the southern region of Iran. A dataset comprising \\n3180 data points was obtained from various drilling sections, \\ninvolving one run of a roller-cone bit and three runs of PDC bits. \\nTo construct the ANN-ROP models, several input variables were \\nconsidered, including depth, rotary speed of the bit, weight on \\nbit (WOB), shut-in pipe pressure, fluid rate, mud weight, the \\nratio of yield point to plastic viscosity, and the ratio of 10-minute \\ngel strength to 10-second gel strength. Three different training \\nfunctions, namely Levenberg-Marquardt (LM), Scaled \\nConjugate Gradient (SCG), and One-Step Secant (OSS), were \\nemployed in combination with the neural networks to estimate \\nthe penetration rates. It was concluded that the ANN-ROP model \\nutilizing the Levenberg-Marquardt (LM) function demonstrated \\nthe best prediction performance, achieving a regression \\ncoefficient (\\n2R ) of 0.91 in training and 0.89 in testing. \\nFurthermore, they also applied the Artificial Bee Colony (ABC) \\nalgorithm to optimize the ROP. The optimization process \\nresulted in an approximate improvement of 20–30% in the rate \\nof penetration. Abdulmalek et al.\\n21 carried out a comparative \\nanalysis between artificial intelligence techniques and some \\ntraditional models for ROP prediction in shaley formations. An \\nartificial neural network was developed for the ROP prediction \\nin the shale formation. The parameters considered for the \\nprediction of the rate of penetration (ROP) included torque, \\nstandpipe pressure, pump rate, mud weight, funnel and plastic \\nviscosities, solid content, and yield point. The traditional ROP \\nmodels such as those proposed by Bingham\\n4, Warren 13, \\nBourgoyne and Young3, Maurer18 and Hareland and Hoberock22, \\nwere selected for comparison. Both the artificial neural network–\\nROP (ANN-ROP) model and the traditional models underwent \\ntraining and testing using a dataset consisting of 347 data points \\nfrom a deep shale formation in an onshore oilfield. Additionally, \\n200 new data points from an upper shale formation were utilized \\nto validate the models. The results indicated that the ANN-ROP'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='from a deep shale formation in an onshore oilfield. Additionally, \\n200 new data points from an upper shale formation were utilized \\nto validate the models. The results indicated that the ANN-ROP \\nmodel outperformed the other models in comprehending the \\nintricate relationships within the data and making accurate \\npredictions. The ANN-ROP model achieved a rate of penetration \\nprediction with an average absolute percentage error (AAPE) of \\n5.776% and a regression coefficient (\\n2R ) of 0.996. Ashrafi et \\nal.23 explored the prediction of rate of penetration (ROP) using \\nvarious optimization algorithms and neural network architectures. \\nThe optimization algorithms employed included Genetic \\nAlgorithm (GA), Particle Swarm Optimization (PSO), \\nBiogeography-based Optimizer (BBO), and Imperialist \\nCompetitive Algorithm (ICA). These algorithms were combined \\nwith different neural network architectures to develop hybrid \\nROP models. To evaluate the performance of the hybrid models, \\nthe results were compared with two other models: Non-linear \\nMultiple Regression (NLMR) and Linear Multiple Regression \\n(LMR) techniques. For the hybrid models, two popular neural \\nnetwork architectures, namely Multi-Layer Perception (MLP) \\nand Radial-Based Function (RBF), were utilized. These \\narchitectures consisted of two hidden layers with 4 and 6 \\nneurons, respectively. The activation function used in the hidden \\nlayers and output layer was tan-sigmoid. The input features were \\nweight on bit, rotational speed of the drill bit, pump inlet flow \\nrate, pore pressure pump pressure, gamma ray, density log, and \\nshear wave velocity. The dataset used for the study consisted of \\n1000 data points, collected from the Marun oilfield in Iran. It \\nwas concluded in their study that the hybrid models utilizing \\nPSO-MLP and PSO-RBF neural networks exhibited the best \\npredictive accuracy for ROP. The root mean square error (RMSE) \\nvalues for these models were 1.12 and 1.4, respectively, \\nindicating their superior performance compared to the other \\ndeveloped models. Iqbal\\n24 developed a mathematical model to \\npredict the rate of penetration (ROP) in drilling operations, \\nconsidering the factors of weight on bit (WOB), bit rotation \\nspeed (RPM), flow rate (FR), formation strength, depth, and \\nformation compaction. He then used his model to optimize the \\ndrilling parameters for a real-time drilling dataset from a Middle \\nEastern oil field, increasing the ROP by 10% and reducing the \\ncost of drilling by 5%. Iqbal’s study demonstrates the importance \\nof using real-time drilling parameters to optimize drilling \\noperations and provides a valuable contribution to the field of \\ndrilling engineering. Burgos et. al.\\n25 developed a convolutional \\nneural network (CNN) model to predict the rate of penetration \\n(ROP) during rotary drilling operations. The model takes in 10 \\ndrilling parameters as inputs, such as weight on bit, rotary speed, \\nflow rate, and hook load. The inputs are normalized between 0 \\nand 1. The CNN architecture consists of 3 convolutional layers \\nfollowed by 2 fully connected layers. The output layer has a \\nsingle node with a linear activation to predict the ROP value. \\nThe model was trained on data from over 600 wells. It achieved \\na mean absolute percentage error (MAPE) of 9.3% on the test \\nset, outperforming traditional machine learning models like \\nlinear regression, random forests, and support vector regression. \\nAn ablation study showed that the CNN’s ability to learn \\ncomplex non-linear relationships between the drilling parameters \\nallowed it to accurately predict ROP, whereas simply averaging \\nthe inputs did not work as well. The model was able to generalize \\nthe data from 50 additional wells, with the MAPE only increasing \\nslightly to 10.2%. This shows the model has good generalization \\nperformance. In conclusion, the CNN approach effectively \\nmodelled the complexity between drilling parameters and ROP,'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='slightly to 10.2%. This shows the model has good generalization \\nperformance. In conclusion, the CNN approach effectively \\nmodelled the complexity between drilling parameters and ROP, \\noutperformed traditional models, and generalized well to new \\ndata. This could enable more efficient drilling operations through \\naccurate ROP predictions. Monazami et al.\\n26 used an artificial \\nneural network (ANN) to predict the rate of penetration (ROP) \\nin drilling operations. The ANN model took cognizance of'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='5\\nOlafadehan OA., et al., J Petro Chem Eng  | V ol: 1 & Iss: 1\\nformation strength, depth, formation compaction, pressure \\ndifferential, bit diameter, weight on bit (WOB), bit rotation \\n(RPM), and bit hydraulics. The authors evaluated the performance \\nof their ANN model on a test dataset of ROP data. They found \\nthat the model was able to predict ROP with high accuracy. The \\naverage error between the predicted and actual ROP values was \\nless than 5%. The model was able to predict ROP with high \\naccuracy, suggesting that ANN is a promising tool for optimizing \\ndrilling parameters and improving the efficiency and cost-\\neffectiveness of drilling operations. Abbas et al.\\n27 employed an \\nartificial neural network (ANN) approach to develop a \\ncomputational-based method for predicting the rate of \\npenetration (ROP). Through a thorough analysis of feature \\nselection, it was determined that out of the 25 input variables \\nexamined, 19 variables had the greatest influence on the ROP. A \\ndataset consisting of 13,125 data points from 14 deviated wells \\nin a formation located in southern Iraq was collected for the \\nstudy. The data specifically pertained to the 8 ½” production \\ncasing section, which was drilled using a drag bit and a \\nconventional bottom hole assembly (BHA) with a water-based \\nmud circulating system. It was concluded that the ROP model \\nbased on the artificial neural network, utilizing three hidden \\nlayers and employing the tan–sigmoid activation function, \\nexhibited the highest efficiency in predicting ROP. The model \\nachieved a regression coefficient of 0.92 during training and \\n0.97 during testing, with mean absolute percentage errors \\n(MAPE) of 9.1% and 8.8% in training and testing, respectively. \\nFurthermore, the model demonstrated good performance on \\nunseen data and did not exhibit overfitting issues. Miyora\\n28 \\nstudied the factors that affect the rate of penetration (ROP) in \\ngeothermal drilling and developed a mathematical model to \\npredict ROP based on these factors. The model includes \\nformation strength, depth, formation compaction, pressure \\ndifferential, bit diameter, weight on bit (WOB), bit rotation \\n(RPM), and bit hydraulics. Miyora () found that all these factors \\nhave a significant impact on ROP and used his model to optimize \\nthe drilling parameters for Well MW-17 in Menengai, Kenya, \\nincreasing the ROP by up to 20%. Al-AbdulJabbar et al.\\n29 utilized \\nan artificial neural network (ANN) in combination with self-\\nadaptive differential evolution (SADE) to predict the rate of \\npenetration (ROP) specifically in horizontal carbonate reservoirs. \\nThe model incorporated six input variables, including rotary \\nspeed, torque, weight on bit, as well as formation petrophysical \\nproperties such as gamma ray, resistivity, and bulk density data. \\nThe developed model demonstrated strong performance, \\nachieving a regression coefficient (\\n2R ) of 0.96 and a mean \\nabsolute percentage error (MAPE) of 5.12%. To further evaluate \\nthe accuracy of the model, an unseen well was used as test data. \\nThe resulting regression coefficient (\\n2R ) and MAPE values \\nwere 0.95 and 5.8%, respectively. Furthermore, their study \\naimed to enhance the interpretability of the ROP model by \\nextracting the weights and biases in a matrix form, effectively \\ntransforming it from a black box model to a white box model. \\nWang et al.\\n30 proposed a hybrid ensemble learning approach for \\npredicting the rate of penetration (ROP) during oil and gas \\ndrilling operations. They argued that existing models for \\npredicting ROP are often inaccurate and unreliable, and that \\nensemble learning methods can provide a more accurate and \\npractical solution. They collected data from drilling operations \\nin the Gulf of Mexico and used it to train and test their hybrid \\nensemble learning model. The model consisted of several \\nmachine learning algorithms, including support vector regression \\n(SVR), random forest regression (RFR), and gradient boosting'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='ensemble learning model. The model consisted of several \\nmachine learning algorithms, including support vector regression \\n(SVR), random forest regression (RFR), and gradient boosting \\nregression (GBR), which were combined using a weighted \\naverage ensemble method. The authors compared the \\nperformance of their hybrid ensemble learning model to other \\nmachine learning models and found that it outperformed these \\nmodels in terms of accuracy and reliability. The authors also \\nconducted sensitivity analyses to determine the most important \\nfeatures for predicting ROP. They found that the weight on bit, \\nthe rotary speed, and the mud flow rate were the most important \\nfeatures for predicting ROP. Liu et al.\\n31 proposed a stacked \\ngeneralization ensemble model for predicting the rate of \\npenetration (ROP) in gas well drilling. The model is trained on a \\ndataset of historical ROP data and drilling parameters from a \\nshale gas survey well in Xinjiang, China. The model combined \\nthe predictions of six machine learning models: support vector \\nregression (SVR), extremely randomized trees (XRT), random \\nforest (RF), gradient boosting machine (GBM), light gradient \\nboosting machine (LightGBM), and extreme gradient boosting \\n(XGB). They first used Pearson correlation analysis to identify \\nthe most important features from the dataset. Then, they used a \\nSavitzky-Golay smoothing filter to reduce noise in the dataset. \\nFinally, they trained the stacked generalization ensemble model \\nusing the leave-one-out cross-validation method. The results \\nshowed that the stacked generalization ensemble model can \\nsignificantly improve the accuracy of ROP prediction. The root \\nmean square error (RMSE) of the model on the testing dataset is \\n0.4853 m/h, which is lower than the RMSE of any of the \\nindividual models. The model also has a high \\n2R value of \\n0.9568. They also used the model to optimize the ROP \\nparameters. They use particle swarm optimization (PSO) to \\nsearch for the optimal combination of ROP parameters. The \\nresults show that the optimized ROP parameters can significantly \\nimprove the ROP. It was thus concluded that the stacked \\ngeneralization ensemble model is a promising approach for \\npredicting ROP in gas well drilling. The model is accurate and \\ncan be used to optimize the ROP parameters. Moraveji and \\nNaderi32investigated the simultaneous effect of six variables on \\npenetration rate using real field drilling data via response surface \\nmethodology (RSM). The important variables included well \\ndepth (D), weight on bit (WOB), bit rotation speed ( N), bit jet \\nimpact force (IF), yield point, \\npY , to plastic viscosity ratio, PVR\\n, ( PVRYp ), 10 min to 10 s gel strength ratio (10MGS/10SGS). \\nEqually, bat algorithm (BA) was used to identify optimal range \\nof factors in order to maximize drilling rate of penetration. Their \\nresults indicated that the derived statistical model provides an \\nefficient tool for estimation of ROP and determining optimum \\ndrilling conditions.\\nThe aim of this study is to analyze the performance of \\nmachine learning and deep learning techniques in predicting the \\nrate of penetration during drilling, which is crucial in optimizing \\ndrilling operations. The results of this study can contribute \\nto drilling planning and optimization of future wells. Exact \\nprediction of the rate of penetration during drilling will save the \\noil and gas industry a large amount of expenses during drilling \\noperation and reduce the amount of non-productive time (NPT) \\nencountered during drilling operation.\\n1.2 Approaches to Rate of Penetration Modelling\\nOver the past few years, a large amount of research has gone \\ninto ways in which ROP can be modelled with its dependent \\ndrilling parameters (controllable and uncontrollable). A key \\ndrive that leads to further research regarding this field is the'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n6\\nnon-comprehensiveness of previous models developed. This \\nis because not all of the known ROP-affecting factors have \\nbeen accounted for in a single model, which has led to poor \\naccuracy and generalizability of the estimated models\\n33. The \\nseemingly large number of factors affecting the ROP and \\nessential requirement for a model with high accuracy and \\nreliable generalization has led to development of various ROP \\nestimation models. An approach to carry out this modelling is \\nhinged on two patterns, which are physics-based approach, and \\ndata-driven approach. The physics-based approach involves \\nthe use of mathematical modelling techniques to evaluate \\nrelationships between dependent parameter (ROP) and the \\nindependent parameters, so as to estimate accurate ROP values. \\nThese mathematical relationships are developed based on the \\nphysics of the borehole. There are various models used for ROP \\nestimation that are created using the physics-based approach \\ne.g., Cunningham model, Bingham\\n4 model, Maurer 19 model, \\nMotahhari et al. model33 and Hareland and Rampersad model 34. \\nThe Cunningham model is given by:\\n          (1)\\nwhere R  is the rate of penetration (ft/h), K  the constant of \\nproportionality, 0W  the threshold weight on bit (lb f) and N  \\nrotary speed (rpm).\\nBingham4 model: \\n  (2)\\nwhere  is the weight-on bit (klb), BD  is the bit diameter \\n(in), a and b are the dimensionless constants for each rock \\nformation.\\nMaurer\\n18 model: \\n   (3)\\nwhere  is the rate of penetration (ft/h), W  the weight (Ibf), s  \\nthe confined rock strength (psi) and D the depth (ft).\\nMotahhari et al. model33: \\n  (4)\\nwhere fw  is the dimensionless wear function, G is a model \\ncoefficient related to bit-rock interactions and bit geometry, \\nα  and γ  are ROP model exponents. The bit coefficient, G, \\nis determined by the bit design, cutter size, cutter rock friction \\ncoefficient and the bit geometry. In this model, a decrease in \\nthe value of the wear function, while keeping other model \\nparameters constant leads to a decrease in ROP. In the case of \\nthe bit size or compressive strength, when its value is decreased \\nan inverse occurs. The relationship between \\nN ,  and R  is \\nnon-linear. Hence, the exponents can yield an optimum value for \\n and N due to the exponential nature of the relationship.\\nHareland and Rampersad Model34: \\n    (5)\\nwhere cN  is the number of cutters and vA  the area of rock \\ncompressed ahead of a cutter (in2 ).\\nOther models used for ROP estimation are as follows:\\nBourgoyne and Young\\n3 model: \\n     (6)\\nwhere  is the weight-on bit (klb), BD  is the bit diamete r \\n(in), a and bare the dimensionless constants for each rock \\nformation.\\n Bourgoyne et al. 35 aimed at seeking to optimize the \\ncontrollable parameters during drilling operation. They proposed \\nthe development of an ROP model based on the application \\nof multiple linear regression technique. The controllable \\nparameters used in developing this model were eight: strength \\nof formation, normal compaction function, weight on bit, \\nbit teeth wear, rotary speed function, bit hydraulic function, \\ndifferential pressure function, and under compaction function. \\nThese parameters were treated as independent parameters on the \\nROP (the dependent parameter). The developed model was then \\napplied to estimate ROP for wells drilled vertically using roller \\ncone bits, and it was concluded that the application of the ROP \\nmodel could help reduce drilling operational cost by 10%. On \\ninception, the model was basically created for modelling ROP \\nfor roller cone bits, but overtime has also shown effectiveness \\nin modelling ROP for PDC bits. The Bourgoyne et al.\\n35 model \\nis given by:\\n∏\\n=\\n=\\n8\\n1i\\niFR            (7)\\nwhere )( 1\\n1\\naeF =  is the formation strength function for \\nBourgyone and Young model, ( )[ ] DaeF −= 10000\\n2\\n2 the normal \\ncompaction function for Bourgyone and Young model, \\n the under compaction function'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='1\\naeF =  is the formation strength function for \\nBourgyone and Young model, ( )[ ] DaeF −= 10000\\n2\\n2 the normal \\ncompaction function for Bourgyone and Young model, \\n the under compaction function \\nfor Bourgyone and Young model, ( )[ ]{ } cpg D aF ρ−= 44 exp  the \\npressure differential function for Bourgyone and Young model, \\n( ) ( )\\n( ) \\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8f9\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n\\uf8ee\\n\\uf8f4\\n\\uf8fe\\n\\uf8f4\\n\\uf8fd\\n\\uf8fc\\n\\uf8f4\\n\\uf8f3\\n\\uf8f4\\n\\uf8f2\\n\\uf8f1\\n\\uf8fa\\n\\uf8fb\\n\\uf8f9\\n\\uf8ef\\n\\uf8f0\\n\\uf8ee\\n−\\n−=\\nt\\nt\\nd w\\nd wd wInaF 4exp 55  the weight on bit function \\nfor Bourgyone and Young model, \\nthe r otary speed function for Bourgyone and Young model, \\n( )[ ]{ } haF −= 77 exp  the bit tooth wear function for Bourgyone \\nand Young model and \\n  the bit hydraulic \\nfunction for Bourgyone and Young model.\\n The physics-based approach has limitations due to the failure \\nto consider all the parameters affecting the drilling operation and \\nin the choice of an empirical constant for the ROP estimation \\nwith respect to the well/borehole in operation. This gave rise \\nto the use of data-driven approaches, which make use of data \\ngenerated during drilling (Logging While Drilling (LWD)) \\nand artificial intelligence techniques for ROP estimation\\n17. The \\napplication of AI models for ROP estimation was suggested by \\nBilgesu et al.\\n10, so as to get over the weakness of the physics-\\nbased approach and improve the accuracy of ROP predictability. \\n2. Methodology\\n2.1 Methods\\nFigure 3 shows the proposed methodology, adopted in this \\nstudy.\\n2.2 Data Collection\\nThe data utilized for this study was obtained from the Daily \\nDrilling Report (DDR) for an oil well in Nigeria. It contains \\nparameters that ROP depends on, which will help make a robust \\nmodel. Such parameters are weight on bit, pump flow rate, mud \\nweight, mud type, drill bit diameter and wellbore trajectory, \\namongst others. After data collection, the uncertainties within \\nthe dataset and the suitable parameters are defined. This leads \\nto filtration of the dataset. The well contains data of 27 columns \\n(the number of variables), 17280 rows, 0% missing cells, and \\n0% duplicate rows.'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='7\\nOlafadehan OA., et al., J Petro Chem Eng  | V ol: 1 & Iss: 1\\n2.3 Data Processing\\nThe data preprocessing phase is also known as Feature \\nEngineering Phase. The data set used for the study is subjected to \\nvarious statistical manipulations and transformations in order to \\nextract relationships and insights between parameters in the data \\nand, process the data into forms that are more understandable \\nby the algorithms, hence, producing better model performances. \\nSuch techniques include exploratory data analysis, missing \\ndata imputation, outlier handling, feature scaling, variable \\ntransformation, and discretization, amongst others. These \\nprocesses help the model match key relationships between the \\ninput parameters and the target variable. In this study, the data \\npreprocessing techniques used were outlier handling, variable \\ntransformation and feature scaling.\\nOutlier handling: This refers to the process involved in dealing \\nwith outliers found in a dataset. Outliers are simply data point \\nthat vary significantly to majority of the dataset. Outliers must \\nbe dealt with since they can significantly affect the outcomes \\nand precision of statistical models. Outlier treatment can be done \\nin a number of ways, such as by removing outliers, capping, \\nor imputing more representative values. The method utilized in \\nthis study was the capping technique, which involved imputing \\nthe interquartile range of the variable with the outlier where the \\noutliers are in the variable. Figures 4 and \\n5 show the box plot of \\nROP data before and after outlier.\\nFigure 4: Box plot of ROP data before outlier.\\nVariable Transformation: This technique was employed in \\nthis study to treat the variables that were skewed either to the \\nleft or to the right. It involves changing a variable’s scale or \\ndistribution to satisfy requirements or enhance the performance \\nof statistical models. This preprocessing technique was \\nperformed on variables that were skewed either to the left or \\nright, so as to equalize variances and establish linearization \\namong the variables, which makes it easier to interpret and \\nmodel. The variable transformation technique use in this study \\nwere LogTransformer and BoxCoxTransformer.\\nFigure 5: Box plot of ROP data after outlier removal. removal.\\nFeature Scaling: This involves changing the scale of \\nnumerical features in a dataset. To make the features similar and \\nprevent some from predominating others based only on their \\ninitial scale, the range or distribution of the features must be \\nchanged. This preprocessing technique is so important as it helps \\nthe machine learning models to better understand the features as \\nthey will usually be within the range of 0 to 1, which the models \\nusually prefer. For some particular models, it is a necessary \\nrequirement to perform feature scaling on the dataset before \\npassing it into them e.g., ANN and CNN, while some models \\nare not influenced when the dataset is scaled or not e.g., Random \\nForest and Extra Trees. There are many types of Feature scaling \\ntechniques e.g., Standard Scaler, MinMax Scaler and Robust \\nscaling. Each of these techniques has their rules of engagement, \\nso as to get better model performance. These rules depend on \\ndataset and model to be used. In this study, the standard scaler \\nwas utilized so as to scale features to have a mean of 0 and a \\nstandard deviation of 1. \\nTable 1 shows the features definition \\nwith the data types used. \\nTable 1: Features definition, unit, and data types.\\nFeature Definition Units Data type\\nDepth The actual depth at which the drilling \\nis taking place.\\nm Numerical\\nLag Depth Time delay or lag between the mea -\\nsured depth and the corresponding \\nROP value.\\nm Numerical\\nWHO Weight on String. klb Numerical\\nROP Rate of Penetration. m/h Numerical\\nRPM  \\nTURBIN\\nTurbine Speed. rev/\\nmin\\nNumerical\\nTorque Rotational force of drill string. klb.ft Numerical\\nSPP Standpipe Pressure. psi Numerical\\nFlow Inflow rate of drilling. fluid pumped into'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='RPM  \\nTURBIN\\nTurbine Speed. rev/\\nmin\\nNumerical\\nTorque Rotational force of drill string. klb.ft Numerical\\nSPP Standpipe Pressure. psi Numerical\\nFlow Inflow rate of drilling. fluid pumped into \\nthe wellbore during drilling.\\ngpm Numerical\\nMw In total volume of drilling mud pumped \\ninto the wellbore during a specific pe-\\nriod of time.\\npcf Numerical\\nMw out total volume of drilling mud pumped \\nout of a wellbore during a specific pe-\\nriod of time.\\npcf Numerical\\nPIT#1 mud pit volume in the first mud pit or \\nmud tank.\\nBbl Numerical\\nPIT#2 mud pit volume in the second mud pit \\nor mud tank.\\nBbl Numerical'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n8\\nPIT#3 mud pit volume in the third mud pit \\nor mud tank.\\nBbl Numerical\\nPIT#4 mud pit volume in the fourth mud pit \\nor mud tank.\\nBbl Numerical\\nPIT#5 mud pit volume in the fifth mud pit or \\nmud tank.\\nBbl Numerical\\nPIT#6 mud pit volume in the Sixth mud pit \\nor mud tank.\\nBbl Numerical\\nTOT ACT Total Actual Time. Bbl Numerical\\nSteel V ol-\\nume\\nThe volume of steel that is used or \\nconsumed during the drilling process.\\nBbl Numerical\\nOver pull Additional force applied to the drilling \\nassembly in order to increase the drill-\\ning efficiency. \\nklb Numerical\\nFlow Pad-\\ndle\\nPercentage of drilling fluid that cir -\\nculates through the wellbore during \\ndrilling.\\n% Numerical\\nBit Posi -\\ntion\\nIt refers to the vertical depth at which \\nthe drilling bit is located within the \\nwellbore.\\nm Numerical\\nHook Po -\\nsition\\nVertical position of the drilling hook \\nor traveling block.\\nm Numerical\\nString  \\nWeight\\nTotal weight of the drill string, includ-\\ning the drill pipe, bottom hole assem -\\nbly (BHA), and any other components \\nattached to it.\\nklb Numerical\\nDrag Resistance encountered by the drill \\nstring and drill bit as they are ad-\\nvanced through the formation.\\nklb Numerical\\nTable 2 gives information on the statistic of ROP variable.\\nTable 2: Descriptive Statistics of ROP variable.\\nStatistic Mean Standard \\ndeviation\\nMini -\\nmum\\n25% 50% 75% Maxi -\\nmum\\nROP 3.964 4.317 0.000 0.000 2.880 7.410 18.525\\nTable 3 gives the Pearson correlation of the oil well features and \\ntheir values.\\nTable 3: Pearson correlation of features with rate of penetration.\\nFeatures Well data\\nDepth -0.37\\nLag depth 0.98\\nWHO 0.15\\nRPM TURBIN0.57\\nTorque 0.70\\nSPP 0.64\\nFlow in 0.57\\nMw in -0.08\\nMw out 0.20\\nPIT#1 0.24\\nPIT#2 -0.14\\nPIT#3 -0.10\\nPIT#4 -0.26\\nPIT#5 0.05\\nPIT#6 0.23\\nTOT ACT-0.25\\nSteel volume 0.20\\nFlow paddle 0.75\\nBit position 0.20\\nHook position 0.26\\nString eight -0.01\\nDrag -0.53\\nThe correlation heat map of the well data is depicted in \\nFigure 6.\\nFigure 6: Correlation heat map of the well data.\\n2.4 Feature Selection\\nThe defined input parameters from the dataset must pass \\nthrough the feature selection phase. The feature selection is \\nthe process of selecting a subset of relevant features (variable, \\npredictors) for usage in building machine learning algorithms. It \\ninvolves selecting the pool of features that has significant impact \\non making prediction with the machine learning algorithm. It is \\na crucial phase, in the bid that a good machine learning model \\nis developed. The feature selection algorithms are divided into \\nthree main categories: filter, wrapper, and embedded methods. \\nThe feature selection helps a user to better interpret the model \\ne.g., a model of 10 input parameters is much easier to interpret \\nthan that of 100 parameters. It also shortens training time for \\nthe machine learning algorithm and enhances generalization \\nby reducing overfitting. In this study, a filter method known \\nas mutual information was used to select optimal features for \\nmodel building. Mutual information is a statistical measure of \\nthe mutual dependence of 2 variables. In other words, mutual \\ninformation quantifies the amount of information gained about \\none random variable through observing another random variable. \\nThe mutual information algorithm is given by: \\n{ }∑∑ ××= )]() (/[) , (log) , () ; ( y px pyx py x pYX I  (8)\\nwhere I is the ranking score, X  and Ythe respective input and'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='9\\nOlafadehan OA., et al., J Petro Chem Eng  | V ol: 1 & Iss: 1\\noutput nodes, x and y are the dependent and target variables \\nrespectively.\\nThe algorithm selects the highest-ranking features that best \\ndescribes the target variable and separates them into percentiles \\ne.g., 10th, 20 th, 30 th etc., depending on the highest ranking. In \\nthis study, the features in the top 50 th (50 percentile), which \\ntranslated to 13 features out of the possible 27, were selected to \\nbe used to the build the machine learning models. The features \\nselected after future selection are depth, lag depth, WOB, SPP, \\nMW IN, PIT#2, PIT#3, PIT#4, PIT#6, TOT ACT, steel volume, \\nbit position and hunk position \\n2.5 Data Splitting\\nData splitting (otherwise known as cross validation) is a \\nprocess utilized in the building of artificial intelligence models. \\nHere, data is partitioned into two or more ways to enable the \\nmodel identify the patterns within the data set and predict its \\nperformance on unseen (real world) data. Two sets of the dataset \\nare created: a training set and a testing set. The training set is \\nused to train the artificial intelligence model on the data while \\nthe testing set is used to assess the model’s performance in real \\nworld scenarios. This is because there is a probability that the \\nbuilt model may not be robust enough to perform successfully on \\nunknown (real world) data. There are various methods used for \\ncross validation operation viz. holdout method, K-fold method, \\nStratified K-fold method, Leave One-Out method, amongst \\nothers. The K-fold cross validation technique was implemented \\nin this study using the Python Sklearn package.\\nA 60:40 split of the oil well data was made into train and \\ntest sets. There are 10368 rows and 13 columns in the training \\nset and 6912 rows and 13 columns in the test set. The training \\nresults were obtained by training the model on the train data \\nand using the resulting model to predict the training set. The \\ntest results were obtained by training the model on the train set \\nbefore predicting the test set.\\n3. Model Development and Training\\nSeven machine learning techniques were analyzed in this \\nwork, to be trained to make predictions of the rate of penetration \\nfor the oil well. The machine learning models that were employed \\nfor this analysis are outlined as follows and their written codes \\ncan be found in the Appendix.\\n3.1 Random Forest Regression\\nRandom forest can be applied to both classification and \\nregression problems. It is an ensemble learning technique that \\ncreates a large number of decision trees during training period \\nand utilizes averaging to improve the prediction accuracy \\nand control over-fitting. Random forests are widely used for \\napplications (such as credit scoring and spam filtering) because \\nthey can handle both categorical and continuous data. During \\ntraining, random forests create a lot of decision trees\\n36. Each \\ntree is constructed using a random subset of the features and \\na sample of the training data. The individual decision trees \\npredictions are combined by the random forest algorithm to \\nprovide a prediction. For a wide range of applications, random \\nforests are a potent and useful machine learning technique. They \\nare often good performers and are quite simple to teach and tune.\\n A schematic of the decision tree regression is depicted in \\nFigure. 7.\\nFigure 7: Decision tree regression schematic37.\\nA random forest model works by training multiple decision \\ntrees in parallel and uses a bagging technique to obtain a robust \\nmodel. Usually, machine learning models have hyperparameters, \\nthat is, parameters in the algorithm that are constant throughout \\ntraining that help the algorithm better understand the data \\npatterns. Hyperparameters in random forest algorithm include \\nmax_depth, max_features, min_samples_leaf, min_samples_\\nsplit, n_estimators. To obtain optimal performance of the \\nrandom forest algorithm, optimal values must be selected for'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='max_depth, max_features, min_samples_leaf, min_samples_\\nsplit, n_estimators. To obtain optimal performance of the \\nrandom forest algorithm, optimal values must be selected for \\nthese hyperparameters. To get the optimal values of the random \\nforest algorithm, a hyperparameter search algorithm (known as \\nthe randomized search algorithm) must be used. This algorithm \\nhelps to generate the optimal hyperparameter value for the \\nhyperparameter to be utilized in the model. After implementing \\nrandomized search algorithm on the well data, the optimal value \\nof the hyperparameters were max_depth = 31, max_features \\n= sqrt, min_samples_leaf = 3, min_samples_split = 13, n_\\nestimators = 666.\\n3.2 Linear Regression\\nLinear regression is a supervised method of machine learning \\nthat uses one or more input features to predict a continuous target \\nvariable. It is assumed that there is a linear relationship between \\nthe input variables and the goal variables. Linear regression \\nis intended to establish the optimal line according to the data, \\nminimizing the difference in predicted and real values. The \\nalgorithm operates by generating the coefficients of the line’s \\nlinear equation. Some hyperparameters in linear regression \\nare copy_X and fit_Intercept. After implementing randomized \\nsearch algorithm on the well data using linear regression as \\nthe base model, the optimal value of the hyperparameters were \\ncopy_X = True, and fit_Intercept = True. \\n3.3 KNearest Neighbor\\nKNearest Neighbor (KNN), as shown in Figure 8, is a \\nsupervised model-based machine learning technique that can \\nbe applied to both classification and regression models. KNN is \\nnot a parametric algorithm, meaning that it does not make any \\nassumptions about the distribution of data. The KNN method \\nis based on the hypothesis that similar occurrences will share \\nsimilar labels. The KNN technique identifies the K closest \\nneighbors to a given data point by reference to a distance metric, \\ntypically Euclidean, and assigns the label to the majority of these \\nK neighbors for a given data point. When the algorithm is doing \\na regression, it takes the weighted average of all the target values \\nfrom the K neighbor and uses it to predict the new value for the \\ngiven data point. The number of neighbors is a hyperparameter \\nthat can be changed. Some hyperparameters in KNN algorithms \\nare algorithm, leaf_size, p, weights, and n_neighbours. After'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n10\\nimplementing randomized search algorithm on the well data \\nusing KNearest neighbor as the base model, the optimal value of \\nthe hyperparameters were algorithm= auto, leaf_size = 10, p = 1, \\nweights = distance and n_neighbours = 3.\\nFigure 8: KNearest Neighbor38.\\nFigure 9: SVM schematic39.\\n3.4 Support Vector Machine Regression\\nSupport vector machine (SVM) regression is a supervised \\nlearning algorithm that is primarily used in classification tasks. \\nIt is derived from the concepts of support vector machines \\n(SVM), as shown in Figure 9. The goal of SVM regression is \\nto identify a function that best matches the relationship between \\nthe input and target variables. The SVM regression generates a \\nhigh-dimensional hyperplane with each data point as a feature \\nvector in the hyperplane space. The objective of the algorithm \\nis to find the hyperplane with the greatest margin, i.e., the \\ndistance from the hyperplane to the nearest data point in each \\nclass. In the regression case, SVM chooses the hyperplane that \\ncontains the most data points within the given range. The range \\nis the margin of tolerance, which allows some data points to fall \\noutside of the range. The support vectors are the data points that \\nfall within or cross the range. Some hyperparameters in support \\nvector regression algorithm are C, epsilon, and kernel. After \\nthe implementation of the randomized search algorithm on the \\nwell data using support vector regression algorithm as the base \\nmodel, the optimal values of the hyperparameters were C = 10, \\nepsilon = 1 and kernel = rbf. \\n3.5 Stacking Technique\\nStacking is a type of machine learning technique, whose \\nalgorithm is shown in Figure 10, that uses the predictive power of \\ndifferent machine learning algorithms to make better predictions \\non datasets. The stacking technique typically involves the use \\nof base models and a meta model. The base models are usually \\ncommon machine learning algorithms such as decision trees, \\nrandom forests, and support vector machines. These base models \\nare trained on a dataset and are used to make predictions; these \\npredictions are then combined in a meta model, which can be \\nlinear regression or a neural network to make final predictions. \\nIt is a powerful machine learning technique since it utilizes the \\ndiverse knowledge of the base models. The base models used for \\nthis study are random forests, or support vector machines, linear \\nregression, and nearest neighbors, while the meta model used is \\nthe linear regression model.\\nFigure 10: Stacking algorithm 40.\\n3.6 Voting Technique\\nV oting is a machine learning technique that involves the \\nintegration of predictions from multiple independent models to \\nform a final prediction, as shown in Figure 11. \\nFigure 11: V oting Algorithm (LevelUpCoding).\\nV oting technique is commonly referred to as ensemble \\nvoting, or majority voting, and is based on the principle that the \\nintegration of the opinions of multiple models can often lead \\nto greater prediction accuracy than the use of a single model. \\nUnder the V oting algorithm, each base model is trained on \\nthe same data set, but with different algorithms or settings. \\nDuring the prediction phase, each base model makes its own \\nprediction based on the data it has been trained on. Finally, the \\nfinal prediction is calculated by adding up all the predictions \\nusing a voting system. The base models used for this study are \\nrandom forests, or support vector machines, linear regression, \\nand nearest neighbors, while the meta model used is the linear \\nregression model.\\n3.7 Convolutional Neural Network\\nConvolution Neural Networks (CNNs) are a type of deep \\nlearning algorithm that is commonly employed in the analysis \\nand interpretation of visual data, including images and videos. \\nCNNs are widely used for image classification, object recognition \\nand image segmentation. However, not only can CNNs be used'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='and interpretation of visual data, including images and videos. \\nCNNs are widely used for image classification, object recognition \\nand image segmentation. However, not only can CNNs be used \\nfor image classification, but they can also be used in regression-\\nbased projects, where it is purposed to predict continuous \\nvariables. A convolution neural network (CNN) usually consists \\nof four components: convolutional layers, pooling layers, fully \\nconnected layers, and output layers, as shown in Figure 12. \\nThese four components usually make for the architecture of \\nCNNs. The main difference between a CNN and a regression-\\nbased CNN is the output layer (output layer) and loss function \\n(loss function). The output layer in a CNN based on regression is \\ndistinct from that of a Softmax-based CNN. Instead of predicting \\nclass probabilities using a function of a Softmax, an output layer \\nis typically composed of an individual neuron with a function'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='11\\nOlafadehan OA., et al., J Petro Chem Eng  | V ol: 1 & Iss: 1\\nof a linear activation. This allows the network to produce a \\ncontinuous value immediately as a regression prediction. For \\nregression tasks, a loss function is often used to measure the \\ndifference between predicted and actual target values. Examples \\nof loss functions that are commonly used include MSE (mean \\nsquared error) and MAE (mean absolute error). \\nFigure 12: Convolutional neural network39.\\n3.8 Model Evaluation\\nThere are metrics usually used to reflect how well the \\nmodel has learnt patterns in the data and the performance of the \\nmodel on the unseen (test) data set. There are metrics used for \\nevaluating the performance of \\nmachine learning models. These \\nmetrics show how far a model’s prediction is from the true \\nvalues. In this study, four error metrics are used to estimate a \\nmodel performance on the learning patterns in the dataset and \\nunseen data (test data). They are the mean absolute error (MAE), \\nroot mean squared error (RMSE), mean squared error (MSE) and \\ncoefficient of determination,\\n2R –score, given by Equations (9) \\nto (12) respectively. \\n∑\\n=\\n−=\\nn\\ni\\ni\\nn\\nyyMAE\\n1\\nˆ              (9)\\n( )∑\\n=\\n−=\\nn\\ni\\ni\\nn\\nyyRMSE\\n1\\n2\\nˆ        (10)\\n∑\\n=\\n−=\\nn\\ni i\\ni\\ny\\nyy\\nnMSE\\n1\\nˆ1               (11)\\n( )\\n( )∑\\n∑\\n=\\n=\\n−\\n−\\n= n\\ni\\ni\\nn\\ni\\nyy\\nyy\\nR\\n1\\n2\\n1\\n2\\n2\\nˆ\\n           (12)\\nwhere yˆ , iy  and y  are the respective predicted, actual and \\nmean values and n the number of observations.\\n4. Results and Discussion\\nThe well data after carrying out various statistical analyses, \\nthe features were reduced from the previous 27 columns to 13 \\ncolumns, , as displayed in Table 4, which is an excerpt of the well \\ndata used for both training and testing. It shows a sample of the \\ndata utilized after feature selection has been carried out, leaving \\n17280 rows and 13 columns. These data were then separated \\nusing cross validation to train and test data respectively. The \\ntrain data contained 10368 rows and 13 columns, while the test \\ndata contained 6912 rows and 13 columns. The linear regression \\nmodel was applied to the training data and test data after the \\noptimal hyperparameters had been generated. The train and \\ntest data were standardized such that data has a mean of 0 and \\nstandard deviation of 1. The results obtained using the linear \\nregression model are presented in Table 5.\\nThe random forest regression, KNearest neighbor, and \\nsupport vector regression (SVR) model were applied to the \\ntraining data and test data after the optimal hyperparameters had \\nbeen generated using the randomized search cv algorithm. The \\ntrain and test data were standardized such that data has a mean \\nof 0 and standard deviation of 1. The random forest, KNearest \\nneighbor, and support vector regression (SVR) models’ results \\nare presented in Tables 6–8 respectively.\\nEqually, the stacking and voting techniques were applied to \\nthe training data and test data after the optimal hyperparameters \\nhad been generated using the randomized search cv algorithm \\nfor the base model used in the technique. The train and test data \\nwere standardized such that data has a mean of 0 and standard \\ndeviation of 1. The results obtained using the stacking and voting \\ntechniques are presented in Tables 9 and 10 respectively.\\nThe convolutional neural network (CNN) model was applied \\nto the training data and test data using an epoch of 120 and a batch \\nsize of 32 together with an output layer of 1. The architecture of \\nthe CNN model created is as follows: two 1–D (one dimensional \\nconvolutional layers), filters (32 and 64), kernel size of two, one \\nGlobal MaxPooling Layer, 5 hidden layers and 1 output layer. \\nThe train and test data were standardized such that data has a \\nmean of 0 and standard deviation of 1. The results obtained \\nusing the CNN model are presented in Table 11.\\nTable 4: Sample taken from well data used to build ML models.\\nTable 5: Linear regression model results.\\nError metric Training data Test data\\nRMSE 2.611 2.565'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='Table 4: Sample taken from well data used to build ML models.\\nTable 5: Linear regression model results.\\nError metric Training data Test data\\nRMSE 2.611 2.565\\nMSE 6.819 6.582\\nMAE 1.773 1.744\\n2R  Score 0.639 0.639\\nTable 6: Random Forest model results.\\nError Metric Training Data Test Data\\nRMSE 0.469 0.676\\nMSE 0.220 0.458\\nMAE 0.207 0.300\\nScore \\n2R\\n0.988 0.975\\nTable 7: KNearest neighbor model results.'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n12\\nError Metric Training Data Test Data\\nRMSE 0.523 0.724\\nMSE 0.309 0.524\\nMAE 0.201 0.239\\nScore\\n 2R\\n0.984 0.971\\nTable 8: SVM model results.\\nError Metric Training Data Test Data\\nRMSE 1.724 1.669\\nMSE 2.972 2.784\\nMAE 0.841 0.832\\nScore 0.843 0.847\\nTable 9: Stacking technique results.\\nError Metric Training Data Test Data\\nRMSE 0.306 0.548\\nMSE 0.033 0.423\\nMAE 0.094 0.300\\nScore \\n2R\\n0.98 0.976\\nTable 10: V oting Technique results.\\nError Metric Training Data Test Data\\nRMSE 0.803 0.826\\nMSE 1.167 1.331\\nMAE 0.646 0.681\\nScore \\n2R\\n0.938 0.926\\nTable 11: CNN results.\\nError Metric Training Data Test Data\\nRMSE 0.797 0.751\\nMSE 1.167 1.331\\nMAE 0.636 0.564\\nScore \\n2R\\n0.924 0.928\\n From the results displayed in Tables 5–11, the stacking \\ntechnique performed better than all the models and techniques \\nemployed in this study for the training data. Hence, the decreasing \\norder of performance of the models for the training data is as \\nfollows: stacking technique > random forest model > KNearest \\nneighbor model > CNN model > V oting technique > SVR model \\n> linear regression model. In terms of the RMSE, the stacking \\ntechnique was 35% better than the random forest model, 41% \\nbetter than the KNearest neighbor model, 62% better than the \\nCNN model and V oting technique, 82% better than the SVR \\nmodel and 88% better than the linear regression model. In terms \\nof the MAE, the stacking technique was 55% better than the \\nRandom Forest model, 53% better than the KNearest Neighbour \\nmodel, 85% better than the CNN model and V oting technique, \\n89% better than the SVR model and 95% better than the linear \\nregression model.\\nFor the testing data, generalizing across the four metrics, \\nthe stacking technique yet again out-performed other models. It \\nwas only in terms of the MAE that the KNearest neighbor model \\noutperformed the stacking technique by 20%, but in terms of the \\nRMSE, MSE, \\n2R  Score, the stacking technique outperformed \\nthe KNearest Neighbour model. SVR model and linear \\nregression model performed better on the test data compared to \\ntheir performances on the train data, indicating generalization of \\nthe models and lack of overfitting on the training data.\\nIn terms of the test (unseen) data, the stacking technique \\nperformed better than all the traditional ML models employed \\nin this study. The next to it on the ranking of the model that \\nbest performed on the test data was the Random Forest model, \\nfollowed by the KNearest Neighbour model, then the CNN \\nmodel, then the V oting technique, then the SVR model and lastly \\nthe linear regression model. \\nIn terms of the RMSE, the stacking technique was 19% better \\nthan the Random Forest model, 24% better than the KNearest \\nNeighbour model, 27% better than the CNN model, 34% better \\nthan the V oting technique, 67% better than the SVR model and \\n79% better than the linear regression model.\\nIn terms of the MAE, the stacking technique and the Random \\nForest model had the same performance score of 0.30. The \\nstacking technique was still 47% better than the CNN model, \\n56% better than the V oting technique, 64% better than the SVR \\nmodel and 83% better than the linear regression model.\\nOur findings in this investigation that the complex ML models \\nof Stacking, V oting and CNN have the capacity to perform better \\nthan the traditional ML model was buttressed in the work of \\nBurgos et al, which was equally corroborated in the study of \\nZhang et. al.\\n19, where the CNN model developed outperformed \\nall the traditional ML models in terms of accuracy and reliability. \\nIt can equally be deduced from this study that irrespective of the \\narchitecture and predictive capacity of the ML model, traditional \\nML models, with proper feature engineering and hyperparameter \\ntuning, can perform better than more complex machine learning \\nmodels.\\n5. Conclusions\\nA comparative analysis of machine learning algorithms in \\npredicting rate of penetration during drilling was carried out in'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='models.\\n5. Conclusions\\nA comparative analysis of machine learning algorithms in \\npredicting rate of penetration during drilling was carried out in \\nthis study. Data was obtained from the Daily Drilling Report \\n(DDR) for an oil well. The well contains data of 17280 rows \\nand 27 columns. The data preprocessing techniques of outlier \\nhandling, variable transformation and feature scaling were \\nemployed. Each of the seven machine learning techniques \\nemployed to predict the rate of penetration during drilling was \\nable to extract meaningful information and patterns from the oil \\nwell data. However, some models outperformed other models by \\na distance, which reflects the predictive power of the algorithms. \\nThe capacity of the stacking algorithm to combine the predictive \\npower of each base model gave it an edge over the rest of the \\nmodels. The voting technique performed well, but not measured \\nup to the performance of the stacking technique. Hence, the \\nstacking technique is a more powerful ensembling technique \\nthan the voting technique. Amongst the base models, the random \\nforest and KNearest Neighbors models are robust since they \\nperformed well on both the train and test data, while the SVM \\nand linear regression models gave the highest errors on both \\nthe train and test data but they also showed their generalization \\ncapability and lower tendency to overfit. The CNN model has \\nthe capacity to perform well on regression-based task like rate \\nof penetration predictions since it performed well on the test and \\ntrain data.\\nStatements and Declarations'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='13\\nOlafadehan OA., et al., J Petro Chem Eng  | V ol: 1 & Iss: 1\\nConflict of interest The authors declare that there is no conflict \\nof interest regarding the publication of this article.\\nFunding The authors received no specific funding for this \\nwork. Hence, the corresponding author confirms that there are \\nno financial and personal relationships with other people or \\norganizations that could inappropriately influence this study.\\n6. References\\n1. Azar HF, Saksala T, Jalali SME. Artificial neural networks models \\nfor rate of penetration prediction in rock drilling. J Structural \\nMechanics 2017;50(3):252-255. \\n2. Rupert JP, Padro CW, Blattel SR. The effects of weight material \\ntype and mud formulation on penetration rate using invert oil \\nsystems. Paper presented at the Society of Petroleum Engineers \\n(SPE) Annual Technical Conference and Exhibition 1981. \\n3. Bourgoyne Jr AT, Young Jr FS. A multiple regression approach \\nto optimal drilling and abnormal pressure detection. SPE J \\n1974;14(04):371-384.\\n4. Bingham MG. A new approach to interpreting rock drillability. \\nTechnical Manual Reprint Oil & Gas Journal 1965: 1-93.\\n5. Elkatatny S. Real time prediction of rheological parameters of \\nKCl water-based drilling fluid using artificial neural networks. \\nArabian Journal for Science and Engineering 2017;42:1655-\\n1665.\\n6. Mahmoud AA, Elkatatny S, Chen W, Abdulraheem A. Estimation \\nof oil recovery factor for water drive sandy reservoirs through \\napplications of artificial intelligence. Energies 2019;12(9):3671.\\n7. Connor Shorten “Machine Learning vs. Deep Learning” Towards  \\nData Science. 2018.\\n8. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature \\n2015;521(7553):436–444.\\n9. Otchere DA, Ganat TOA, Gholami R, Ridha S. Application \\nof supervised machine learning paradigms in the prediction \\nof petroleum reservoir properties: Comparative analysis of \\nANN and SVM models. Journal of Petroleum Science and \\nEngineering 2021;200:108182. \\n10. Bilgesu HI, Tetrick LT, Altmis U, Mohaghegh S, Ameri S. A new \\napproach for the prediction of rate of penetration (ROP) values. \\nPaper presented at the Society of Petroleum Engineers (SPE) \\nEastern Regional Meeting 1997; SPE–39231–MS.\\n11. Arabjamaloei R, Shadizadeh S. Modeling and optimizing rate \\nof penetration using intelligent systems in an Iranian southern \\noil field (Ahwaz oil field). Petroleum Science and Technology \\n2011;29(16):1637–1648.\\n12.  Bataee M, Mohseni S. Application of artificial intelligent systems \\nin ROP optimization: a case study in Shadegan oil field. Paper \\npresented at the Society of Petroleum Engineers (SPE) \\nMiddle East Unconventional Gas Conference and Exhibition \\n2011;SPE-140029-MS.\\n13. Warren TM. Penetration-rate performance of roller-cone bits. \\nSPE Drill Eng 1987;2(01):9–18.\\n14. AL-Mahasneh MA. Optimization Drilling Parameters \\nPerformance during Drilling in Gas Wells. International Journal \\nof Oil, Gas and Coal Engineering 2017;5:19-26. \\n15. Amar K, Ibrahim, A. Rate of penetration prediction and \\noptimization using advances in artificial neural networks, a  \\ncomparative study. In Proceedings of the 4th International Joint \\nConference on Computational Intelligence 2012;1:647-652.\\n16. Shi X, Liu G, Gong X, Zhang J, Wang J, Zhang H. An efficient \\napproach for real-time prediction of rate of penetration in offshore \\ndrilling. Mathematical Problems in Engineering 2016;(Article ID \\n3575380):1–13.\\n17. Ahmed A, Elkatatny S, Abdulraheem A, Mohammed M, Ali A , \\nMohamed I. Prediction of rate of penetration of deep and tight \\nformation using support vector machine. In Proceedings of the \\nSPE Kingdom of Saudi Arabia Annual Technical Symposium \\nand Exhibition, Dammam, Saudi Arabia. 2018; SPE–192316–\\nMS.\\n18. Maurer WC. The, “perfect-cleaning” theory of rotary drilling. J \\nPet Technol 1962;14(11):1270-1274.\\n19. Zhang Y, Zhang X, Chen Y. Deep neural networks for predicting \\nrate of penetration in drilling. Journal of Petroleum Science and \\nEngineering 2018;165:734-743.'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='Pet Technol 1962;14(11):1270-1274.\\n19. Zhang Y, Zhang X, Chen Y. Deep neural networks for predicting \\nrate of penetration in drilling. Journal of Petroleum Science and \\nEngineering 2018;165:734-743.\\n20.  Zhao Y, Noorbakhsh A, Koopialipoor M, Azizi A, Tahir MM. \\nA new methodology for optimization and prediction of rate \\nof penetration during drilling operations. Engineering with \\nComputers 2020;36:587-595.\\n21. Abdulmalek A, Abdulwahab A, Salaheldin E, Abdulazeez A. New \\nartificial neural networks model for predicting rate of penetration \\nin deep shale formation. Sustainability 2019;11(22): 6527. \\n22. Hareland G, Hoberock LL. Use of drilling parameters to predict \\nin-situ stress bounds. Paperpresented at the SPE/IADC Drilling \\nConference. Netherlands 1993:SPE-25727-MS. \\n23. Ashrafi SB, Anemangely M, Sabah M, Ameri MJ. Application of \\nhybrid artificial neural networks for predicting rate of penetration \\n(ROP): a case study from Marun oil field. Journal of Petroleum \\nScience and Engineering 2019;175:604-623.\\n24. Iqbal F. Drilling optimization technique using real time \\nparameters. SPE Russian Oil & Gas Technical Conference and \\nExhibition, Moscow, Russia, 2008. \\n25. Burgos CE, Zhang T, Li J, Zhang C, Chen S. ROP prediction \\nusing convolutional neural networks for Paleozoic shale drilling. \\nJournal of Petroleum Science and Engineering 2019;17:633-\\n641.\\n26. Monazami M, Hashemi A, Shahbazian M. Drilling rate of \\npenetration prediction using artificial neural network: A case \\nstudy of one of Iranian Southern oil fields. Journal of Oil and \\nGas Business 2012.\\n27. Abbas AK, Rushdi S, Alsaba M, Al Dushaishi MF. Drilling rate of \\npenetration prediction of high-angled wells using artificial neural \\nnetworks. J. Energy Resour. Technol 2019;141(11):112904.\\n28. Miyora TO. 2014. Modeling and optimization of geothermal \\ndrilling parameters: A case study of well MW-17 in Menengai \\nKenya, MS Thesis. University of Iceland 2014.\\n29. Al-AbdulJabbar A, Elkatatny S, Mahmoud AA, et al. Prediction \\nof the rate of penetration while drilling horizontal carbonate \\nreservoirs using the self-adaptive artificial neural networks \\ntechnique. Sustainability 2020;12(4):1376.\\n30. Wang K, Zhang Y, Zhang X, Wang Y. A hybrid ensemble \\nlearning approach for rate of penetration prediction in oil and \\ngas drilling. Journal of Petroleum Science and Engineering \\n2020;194:107424.\\n31.  Liu N, Gao H, Zhen Z, Hu Y, Duan L. A stacked generalization  \\nensemble model for optimization and prediction of the gas well  \\nrate of penetration: a case study in Xinjiang. Journal of Petroleum \\nExploration and Production Technology 2021;6:1595-1608.\\n32. Moraveji MK, Naderi M. Drilling rate of penetration prediction \\nand optimization using response surface methodology and bat \\nalgorithm. Journal of National Gas Science and Engineering \\n2016;31:829–841. \\n33. Motahhari HR, Hareland G, Nygaard R, Bond B. Method of \\noptimizing motor and bit performance for maximum ROP. J Can \\nPet Technol 2009;48(06):44-49. \\n34. Hareland G, Rampersad PR. Drag - Bit Model Including Wear. \\nAmerica/Caribbean Petroleum Engineering Conference 1994: \\nSPE-26957-MS.\\n35. Bourgoyne Jr AT, Millheim KK, Chenevert ME, Young Jr FS.'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n14\\nApplied drilling engineering. SPE Textbook Series 1991;2:ISBN: \\n978-1-55563-001-0.\\n36. Quinlan JR. Induction of decision trees. Machine Learning \\n1986;1(1):81-106. \\n37. SametGirgin, Decision Tree Regression in 6 Steps with Python, \\nPursuitData (Medium). 2019.\\n38. Javat (2022).  \\n39. Pandey YN, Rastogi A, Kainkaryam S, Bhattacharya S, Saputelli \\nL. Overview of Machine Learning and Deep Learning Concepts. \\nMachine Learning in the Oil and Gas Industry 2020:75-152.\\n40. GeeksForGeeks (2022)\\n7 Appendix: Codes for the different algorithms employed in this work.\\nRandom Forest Algorithm\\nLinear Regression Algorithm\\nK Nearest Neighbour Algorithm'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='15\\nOlafadehan OA., et al., J Petro Chem Eng  | V ol: 1 & Iss: 1\\nSVR Algorithm\\nStacking Algorithm\\nVoting Algorithm'),\n",
              " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20231110215223Z00'00'\", 'moddate': \"D:20231110215223Z00'00'\", 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'Comparative_Analysis_of_Machine_Learning.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='J Petro Chem Eng | V ol: 1 & Iss:1Olafadehan OA., et al.,\\n16\\nCNN Algorithm Code')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p = PyPDFLoader('Comparative_Analysis_of_Machine_Learning.pdf')\n",
        "docs = p.load_and_split()\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu2UauiC4Rpr"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eqpM6I854Rpr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F21690EF80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F21690FDC0>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "api_key=os.getenv(\"GROQ_API\")\n",
        "llm=ChatGroq(groq_api_key=api_key,model=\"llama-3.3-70b-versatile\")\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "token=os.getenv(\"astra_token\")\n",
        "a_id = os.getenv(\"astra_id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "cassio.init(token=token, database_id=a_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\NEW USER\\anaconda3\\envs\\genv\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NEW USER\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "astra_store = Cassandra(embedding=embeddings,table_name='ik_db',session=None,keyspace=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "splitter = CharacterTextSplitter(separator='/n',chunk_size=1000,chunk_overlap=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "docss= splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "astra_store.add_documents(docss)\n",
        "vec = VectorStoreIndexWrapper(vectorstore=astra_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dSaUPguw389l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'According to the text, the following machine learning techniques were used:\\n\\n1. Random Forest\\n2. Support Vector Machine (SVM)\\n3. Linear Regression\\n4. K-Nearest Neighbors (KNN)\\n5. Stacking Technique (which combines the predictions of multiple models)\\n6. Voting Technique (which integrates the predictions of multiple models)\\n7. Convolutional Neural Network (CNN)\\n\\nSo, in total, 7 machine learning techniques were used in this paper.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec.query('How many machine learning where used in this paper',llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i am david, i am good'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "h = ['i am david, i am good']\n",
        "'/n'.join(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
